---
title: "Metropolis-Hastings"
author: "Ngoc Duong"
date: "5/5/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(tidyverse)
library(invgamma)
library(mvtnorm)
library(lubridate)
```

```{r traintest}
raw_dat = read.csv(file.path(getwd(), "hurrican356.csv")) %>%
  janitor::clean_names() %>%
  mutate(id = str_sub(id, end = -6L)) %>%
  group_by(id) %>%
  mutate(n = 1,
         t = cumsum(n) - 1) %>% # create t variable grouped by hurrican
  select(-n)

## 80-20 train/test data split ***by hurricane***
hurricane_names = unique(raw_dat$id) # unique hurricanes

set.seed(1)
ind = sample(hurricane_names, size = length(hurricane_names)*0.8) # sample hurricanes with probability 0.8 for training
train = raw_dat %>% # data for train hurricanes
  filter(id %in% ind)
test = raw_dat %>% # data for test hurricanes
  filter(!(id %in% ind))
```


```{r}
train_clean = train %>% rename(year = season) %>% 
  separate(time, into = c("date", "hour"), sep = " ") %>% 
  mutate(hour = str_replace(hour, ":00:00\\)", ""),
         hour = as.numeric(hour),
         date = str_replace(date, "\\(", ""),
         date = yday(date),
         nature = as.numeric(as.factor(nature))) %>% 
  group_by(id) %>% 
  mutate(n_latitude = lag(latitude,1),
         n_longitude = lag(longitude,1),
         n_windkt = lag(wind_kt,1),
         diff_long = c(NA, diff(longitude)),
         diff_lat = c(NA, diff(latitude)),
         diff_windkt = c(NA, diff(wind_kt))) %>% 
  ungroup() %>% 
  na.omit() %>% 
  select(id, wind_kt, n_windkt, date, year, nature, diff_lat, diff_long, diff_windkt)

train_test = filter(train_clean, id == "ALLISON")
  
XX = train_test %>% mutate(intercept = 1) %>% 
  select(intercept, date:diff_windkt) %>% as.matrix()

YY = train_test %>% select(wind_kt) %>% as_vector()
```



### Find log-likelihood 
Betas ~ indep MVN, rho ~ truncated normal, Sigma ~ inverse-gamma

```{r}
## Sigma is a vector of beta_k variances
## X is vector of observations
## mu_X??
## rho is scalar

ll_f = function(X, Y, beta, Sigma, rho) {
  invsigma = diag(1/Sigma)
  mu_X = t(X) %*% diag(beta) + rho*Y

  a = log(Sigma)
  b = t(X - t(mu_X)) %*% invsigma %*% (X - t(mu_X))
  c = 7*log(2*pi)
  
  l = -0.5*(as.vector(a + b) + c)
  return(l)
}


ll_beta = function(Sigma, X) {
  invsigma = diag(1/Sigma)
  
  a = log(Sigma)
  b = t(X) %*% invsigma %*% X
  c = 7*log(2*pi)
  
  l = -0.5*(as.vector(a + b) + c)
  return(l)
}

ll_rho = function(rho) {
  a = log(sqrt(5))
  b = log(dnorm(sqrt(5)*(rho - 0.5)))
  c = log(pnorm(sqrt(5)/2) - pnorm(-sqrt(5)/2))
  
  l = a + b - c
  return(l)
}

ll_invSigma = function(Sigma) {
  invsigma = diag(1/Sigma)

  a = 0.001*log(0.001)
  b = log(factorial(0.001 - 1))
  c = 1.001*log(invsigma)
  d = 0.001/invsigma
  
  l = a - b + c + d
  return(diag(l))
}


logpost_all = function(X, Y, beta, rho, Sigma){    
  return(ll_beta(Sigma, X) + ll_invSigma(Sigma) + ll_f(X, Y, beta, Sigma, rho))
}
```


### MH algorithm

```{r}

MH_step = function(train_X, train_Y, params, avec){
  
  # cast X as matrix 
  train_X = unname(train_X) %>% as.matrix

  # initialize parameter vectors
  bb = c() # beta vec
  rr = NA # rho
  ss = c() # Sigma
  
  p_betas = cur_betas = params[1:7]
  cur_rho = rho = params[8]
  cur_sigma = params[9:15]
  
  # estimate Betas
  for (j in 1:7) {
    num = logpost_all(train_X, train_Y, beta = p_betas, rho = cur_rho, Sigma = cur_sigma)
    denom = logpost_all(train_X, train_Y, beta = cur_betas, rho = cur_rho, Sigma = cur_sigma)
    
    cur_betas = p_betas
    p_betas[j] = cur_betas[j] + ((runif(1) - 0.5) * 2 * avec[j])
    
    ## Assign posterior values (component-wise, since logpost_all gives vector)
    if (log(runif(1)) < (num[j] - denom[j])) {
      bb[j] = p_betas[j]
    } else {
      bb[j] = cur_betas[j]
    }
  }
  
  # placeholder for posterior rho and Sigma values
  #sampling rhos
  p_rho = cur_rho#truncnorm::dtruncnorm(rho, a = 0, b = 1, mean = 0.5, sd = 1/5)/avec[8]
  
  #sampling sigmas
  p_sigma = cur_sigma #invgamma::rinvgamma(1, 0.001, 0.001)/avec[9:15]
    
  rr = p_rho
  ss = p_sigma

  
  return(c(bb, rr, ss))
}

set.seed(1)
MH_step(train_X = XX[1, ], # estimates must be made for each row (hour t)
        train_Y = YY[1], 
        params = p_start, 
        avec = avec)


```

Run MCMC

```{r}
# first, specify the avecs and parameter starting values
b_avec = rep(2, 7)
r_a = 2
sig_a = rep(50, 7)
avec = c(b_avec, r_a, sig_a)

beta_start = rep(0, 7)
sigma_start = rep(1, 7)
rho_start = 0.7
p_start = c(beta_start, rho_start, sigma_start)

nrep = 1e3
mchain <- matrix(NA, nrow = nrep, ncol = 15)
mchain[1, ] <- p_start

for (i in 2:nrep) {
  mchain[i, ] = MH_step(train_X = XX[1, ], # estimates must be made for each row (hour t)
                        train_Y = YY[1], 
                        params = mchain[i - 1, ], 
                        avec = avec)
}



```


We can also discard the first 501 values in the chain as “burnin”. This is a way to select only the last 501 values for betas and rhos 

```{r}
#run the chain
burned = mchain[501:1000, ]
```

Next, we might first check the probability of accepting the proposals for each parameter in order to tune the choice ofa. Here’s a useful utility for calculating this:

```{r}
numunique <- function(mat){
  for(i in 1:ncol(mat)) {
    cat(i,"\t",length(unique(mat[,i])),"\n")
  }
}

#for betas
numunique(mchain[, 1:7])

#for rhos
numunique(mchain[, 8])

#for sigma?
numunique(mchain[, 9:15])
```


