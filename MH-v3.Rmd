---
title: "MH algorithm--version 2"
author: "Ngoc Duong"
date: "5/8/2020"
output: github_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(tidyverse)
library(invgamma)
library(mvtnorm)
library(lubridate)
library(truncnorm)
```

```{r traintest}
raw_dat = read.csv(file.path(getwd(), "hurrican356.csv")) %>%
  janitor::clean_names() %>%
  mutate(id = str_sub(id, end = -6L)) %>%
  rename(year = season) %>% 
  separate(time, into = c("date", "hour"), sep = " ") %>%
  mutate(hour = str_replace(hour, ":00:00\\)", ""),
         hour = as.numeric(hour),
         date = str_replace(date, "\\(", ""),
         date = yday(date),
         nature = as.numeric(as.factor(nature))) %>%
  mutate(unique_id = group_indices(., id, year)) %>%
  group_by(unique_id) %>%
  mutate(n = 1,
         t = cumsum(n) - 1) %>% # create t variable grouped by hurrican
  select(-n)

## 80-20 train/test data split ***by hurricane***
hurricane_names = unique(raw_dat$unique_id) # unique hurricanes

set.seed(1)
ind = sample(hurricane_names, size = length(hurricane_names)*0.8) # sample hurricanes with probability 0.8 for training
train = raw_dat %>% # data for train hurricanes
  filter(unique_id %in% ind)
test = raw_dat %>% # data for test hurricanes
  filter(!(unique_id %in% ind))
```


```{r}
train_clean = train %>%
  group_by(unique_id) %>% 
  mutate(diff_lat = latitude - lag(latitude, 6),
         diff_long = longitude - lag(longitude, 6),
         diff_windkt = wind_kt - lag(wind_kt, 6)) %>% 
  ungroup() %>% 
  dplyr::select(unique_id, wind_kt, date, year, nature, diff_lat, diff_long, diff_windkt) %>% 
  na.omit()

train_test = train_clean %>% filter(unique_id == 1)
  
XX = train_test %>% 
  mutate(intercept = 1) %>% 
  select(intercept, date:diff_windkt) %>% 
  as.matrix()

YY = train_test %>% select(wind_kt) %>% as_vector()
```



### Find log-likelihood 
Betas ~ indep MVN, rho ~ truncated normal, Sigma ~ inverse-gamma

```{r}
## Sigma is a vector of beta_k variances
## X is vector of observations
## mu_X??
## rho is scalar
ll_f = function(X, Y, beta, Sigma, rho) {
  sig.vec = rep(Sigma, 7)
  invsigma = diag(1/sig.vec)
  mu_X = t(X) %*% diag(beta) + rho*Y
  
  a = log(Sigma)
  b = t(X - t(mu_X)) %*% invsigma %*% (X - t(mu_X)) %>% as.numeric
  c = 7*log(2*pi)
  
  l = -0.5*(as.vector(a + b) + c)
  return(l)
}
# ll_f(xx1, YY[10], beta = rep(0, 7), Sigma = 1, rho = 0.5)
# ll_beta = function(Sigma, X) {
#   invsigma = diag(1/Sigma)
#   
#   a = log(Sigma)
#   b = t(X) %*% invsigma %*% X
#   c = 7*log(2*pi)
#   
#   l = -0.5*(as.vector(a + b) + c)
#   return(l)
# }
###################################
ll_rho = function(rhos) {
  # Prior for correlation rho between time points
  prob = dtruncnorm(rhos, a = 0, b = 1, mean = 0.5, sd = sqrt(0.2))
  return(log(prob))
}
ll_invsigma = function(sigma) {
  # Prior for the inverse of the error covariance matrix
  return(log(dinvgamma(1/sigma, shape = 0.0001, rate = 0.0001)))
}
ll_beta = function(betas, sigma) {
  return((log(dnorm(betas, mean = 0, sd = sqrt(sigma)))))
}
###################################
# ll_rho = function(rho) {
#   a = log(sqrt(5))
#   b = log(dnorm(sqrt(5)*(rho - 0.5)))
#   c = log(pnorm(sqrt(5)/2) - pnorm(-sqrt(5)/2))
#   
#   l = a + b - c
#   return(l)
# }
# ll_invSigma = function(Sigma) {
#   invsigma = diag(1/Sigma)
#   a = 0.001*log(0.001)
#   b = log(factorial(0.001 - 1))
#   c = 1.001*log(invsigma)
#   d = 0.001/invsigma
#   
#   l = a - b + c + d
#   return(as.numeric(l))
# }
logpost_all = function(X, Y, beta, rho, Sigma){  
  post_beta = ll_beta(beta, Sigma) + ll_invsigma(Sigma) + ll_f(X, Y, beta, Sigma, rho)
  post_rho = ll_rho(rho) + ll_f(X, Y, beta, Sigma, rho)
  post_sigma = ll_invsigma(Sigma) + ll_f(X, Y, beta, Sigma, rho)
  
  return(c(post_beta, post_rho, post_sigma))
}
xx1 = unname(XX[10,]) %>% as.matrix
# 
logpost_all(xx1, YY[10], beta = mchain[4, 1:7], rho = 0.2, Sigma = -2)
# 
# ll_rho(0.5)
# 
# ll_f(xx1, YY[10], beta = rep(0, 7), Sigma = 1, rho = 0.5)
```


### MH algorithm

```{r}
MH_step = function(train_X, train_Y, params, b_avec, r_a, sig_a){
  
  # cast X as matrix 
  train_X = unname(train_X) %>% as.matrix

  # initialize vectors for posterior estimates
  bb = c() # beta vec
  rr = NA # rho
  ss = NA # Sigma
  
  # current values 
  p_betas = cur_betas = params[1:7]
  cur_rho = params[8]
  cur_sigma = params[9]
  
  ### Component-wise MH 
  # estimate Betas
  bnum = vector(length = 7)
  bdenom = vector(length = 7)
  for (j in 1:7) {
    cur_betas[j] = p_betas[j]
    p_betas[j] = cur_betas[j] + (runif(1) - 0.5) * 2 * b_avec[j] # use random walk to sample next value
    
    bnum = logpost_all(train_X, train_Y, beta = p_betas, rho = cur_rho, Sigma = cur_sigma)
    bdenom = logpost_all(train_X, train_Y, beta = cur_betas, rho = cur_rho, Sigma = cur_sigma)
    if (log(runif(1)) < (bnum[j] - bdenom[j])) {
      bb[j] = p_betas[j]
    } else {
      bb[j] = cur_betas[j]}
  }
  
  #sampling rhos
  p_rho = cur_rho + (runif(1) - 0.5) * 2 * r_a
  rnum = logpost_all(train_X, train_Y, beta = bb, rho = p_rho, Sigma = cur_sigma)
  rdenom = logpost_all(train_X, train_Y, beta = bb, rho = cur_rho, Sigma = cur_sigma)
  #cur_rho = p_rho
  if (log(runif(1)) < (rnum[8] - rdenom[8])) {
    rr = p_rho
  } else {
    rr = cur_rho
  }
  
  #sampling sigmas
  p_sigma = cur_sigma + (runif(1) - 0.5) * 2 * sig_a
  
  # if (p_sigma < 0) {
  #   snum = Inf
  # } else {
  #   snum = logpost_all(train_X, train_Y, beta = bb, rho = rr, Sigma = p_sigma)
  # }
  snum = logpost_all(train_X, train_Y, beta = bb, rho = rr, Sigma = p_sigma)
  sdenom = logpost_all(train_X, train_Y, beta = bb, rho = rr, Sigma = cur_sigma)
  if (log(runif(1)) < (snum[9] - sdenom[9])) {
    ss = p_sigma
  } else {
    ss = cur_sigma}
  
  return(c(bb, rr, ss))
}

numunique <- function(mat){
  apply(mat, 2, function(i){
    length(unique(i))
  })
}
  
```

Run MCMC

```{r}
# first, specify the avecs and parameter starting values
#b_avec = c(0.3, rep(0.7,6)) #c(10, rep(2, 5), 5)
#avec = c(b_avec, r_a, sig_a)
beta_start = rep(0, 7)
sigma_start = 1
rho_start = 0.5
p_start = c(beta_start, rho_start, sigma_start)

nrep = 2000
bnd = sort(nrep - (0:((nrep/2) - 1))) # second half index (after burn-in)
mchain <- matrix(NA, nrow = nrep, ncol = 9)
mchain[1, ] <- p_start

# MH_step(train_X = XX[1, ], # estimates must be made for each row (hour t)
#                         train_Y = YY[1], 
#                         params = mchain[1, ], 
#                         b_avec = c(5, 0.2, 0.01, rep(1.1, 4)),
#                         r_a = 0.1,
#                         sig_a = 1)

for (i in 2:nrep) {
  mchain[i, ] = MH_step(train_X = XX[15, ], # estimates must be made for each row (hour t)
                        train_Y = YY[15], 
                        params = mchain[i - 1, ], 
                        b_avec = c(6.5, 0.1, 0.009, 5, 6, 3.5, 1), #rep(3.5, 4)),
                        r_a = 0.1,
                        sig_a = 1)
}

as_tibble(mchain[bnd, ]) %>% 
  mutate(n = 1:nrow(.)) %>% 
  pivot_longer(V1:V9, 
               names_to = "Predictors",
               values_to = "estimate") %>% 
  ggplot(aes(x = n, y = estimate, group = Predictors, col = Predictors)) + 
  geom_line() +
  facet_grid(Predictors~., scales = "free")

numunique(mchain[bnd, ])
colMeans(mchain[bnd, ])


```

Loop

```{r}
XX = train_clean %>% mutate(intercept = 1) %>% 
  select(intercept, date:diff_windkt) %>% as.matrix()

YY = train_clean %>% select(wind_kt) %>% as_vector()

nrep = 2000
burned = matrix(nrow = nrow(train_clean), ncol = 9)
bnd = sort(nrep - (0:((nrep/2) - 1)))

for (k in 1:nrow(train_clean)){
  for (i in 2:nrep) {
  mchain[i, ] = MH_step(train_X = XX[k, ], # estimates must be made for each row (hour t)
                        train_Y = YY[k], 
                        params = mchain[i - 1, ], 
                        b_avec = c(0.9, 0.1, rep(0.005, 5)),
                        r_a = 0.05,
                        sig_a = 0.75)}
    burned[k,] = apply(mchain[bnd, ], 2, mean)
}
```


Parallel computing

```{r}
library(doParallel)
library(parallel)

nrep = 2000
bnd = sort(nrep - (0:((nrep/2) - 1)))

cl <- parallel::makeCluster(4)
doParallel::registerDoParallel(cl)

foreach(k = 1:nrow(train_clean), .combine = rbind, .packages = c("tidyverse","invgamma","truncnorm","mvtnorm", "lubridate")) %dopar% {
  for (i in 2:nrep) {
  mchain[i, ] = MH_step(train_X = XX[k, ],
                        train_Y = YY[k], 
                        params = mchain[i - 1, ], 
                        b_avec = c(0.9, 0.1, rep(0.005, 5)),
                        r_a = 0.05,
                        sig_a = 0.75)}
    burned = apply(mchain[bnd, ], 2, mean)
}

parallel::stopCluster(cl)
```


**Calculate prediction error**

Prepare test data

```{r}
test_clean = test %>%
  group_by(unique_id) %>% 
  mutate(diff_lat = latitude - lag(latitude, 6),
         diff_long = longitude - lag(longitude, 6),
         diff_windkt = wind_kt - lag(wind_kt, 6)) %>% 
  ungroup() %>% 
  dplyr::select(unique_id, wind_kt, date, year, nature, diff_lat, diff_long, diff_windkt) %>% 
  na.omit() %>% mutate(intercept = 1) %>% 
  select(intercept, date:diff_windkt, wind_kt) %>% 
  as.matrix()
```

Assume we already obtained a vector of estimated paramaters: param

```{r}
# prediction function
pred = function(data, param){
  return(data[,c(1:7)] %*% param[1:7] + param[8]*(as.numeric(data[,"wind_kt"])) + rnorm(1, mean = 0, sd = param[9]))
}

#predict wind speed at t+6
pred_y = pred(test_clean, param)
```

Obtain vector of true y(t+6) (need to revise if these are the correct position of y(t+6)'s)

```{r}
test_true_y = test %>%
  group_by(unique_id) %>% 
  mutate(diff_lat = latitude - lag(latitude, 6),
         diff_long = longitude - lag(longitude, 6),
         diff_windkt = wind_kt - lag(wind_kt, 6)) %>% 
  dplyr::select(wind_kt) %>% 
  slice(-c(1:6)) %>% 
  ungroup() %>% 
  na.omit()

test_true_y = as.matrix(test_true_y)[,2]
```

MSE and RMSE
```{r}
#find MSE 
MSE = mean((pred_y-test_true_y)^2)

#RMSE 
RMSE = sqrt(MSE)
```


